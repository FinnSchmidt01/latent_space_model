{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/srv/user/turishcheva/sensorium_replicate/sensorium_2023/\")\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nnfabrik.utility.nn_helpers import set_random_seed\n",
    "from torch.distributions.kl import kl_divergence\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "set_random_seed(seed)\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "from neuralpredictors.layers.cores.conv2d import Stacked2dCore\n",
    "from neuralpredictors.layers.encoders.mean_variance_functions import \\\n",
    "    fitted_zig_mean\n",
    "from neuralpredictors.layers.encoders.zero_inflation_encoders import ZIGEncoder\n",
    "from neuralpredictors.measures import modules, zero_inflated_losses\n",
    "from neuralpredictors.training import early_stopping\n",
    "from nnfabrik.utility.nn_helpers import set_random_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eval import eval_model\n",
    "from moments import load_mean_variance\n",
    "from sensorium.datasets.mouse_video_loaders import mouse_video_loader\n",
    "from sensorium.models.make_model import make_video_model\n",
    "from sensorium.utility import scores\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "from sensorium.utility.scores import get_correlations\n",
    "# wandb.login('0066bbfe063ba7e14bd2b068d18ae8ab2d6299ed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_gaussian(mu, sigma):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence D_KL(q || p) where:\n",
    "    q(z) = N(mu, diag(sigma^2)) and p(z) = N(0, I)\n",
    "\n",
    "    Parameters:\n",
    "    - mu (torch.Tensor): Mean vector of the Gaussian distribution q(z), shape (B,time,hidden).\n",
    "    - sigma (torch.Tensor): Standard deviation vector of q(z), not squared, shape (B,time,).\n",
    "\n",
    "    Returns:\n",
    "    - kl_div (torch.Tensor): The KL divergence.\n",
    "    \"\"\"\n",
    "\n",
    "    sigma_squared = sigma**2\n",
    "    inverse_sigma_squared = 1.0 / sigma_squared\n",
    "\n",
    "    # Trace term: Sum of inverse variances (since trace of a diagonal matrix is the sum of its diagonal elements)\n",
    "    dim = mu.shape[0] * mu.shape[1] * mu.shape[2]\n",
    "\n",
    "    trace_term = (\n",
    "        inverse_sigma_squared * dim\n",
    "    )  # sigma is a scalar which is constant acrosss all dims\n",
    "\n",
    "    # Quadratic term: (mu^T * Sigma_0^{-1} * mu)\n",
    "    quadratic_term = torch.sum(mu**2) * inverse_sigma_squared\n",
    "\n",
    "    # Log-determinant term: 2 * sum(log(sigma))\n",
    "    log_det_term = (\n",
    "        2 * torch.log(sigma) * dim\n",
    "    )  # sigma is a scalar which is constant acrosss all dims\n",
    "\n",
    "    kl_div = 0.5 * (trace_term + quadratic_term - dim + log_det_term)\n",
    "    return kl_div\n",
    "\n",
    "\n",
    "# compute exponentail moving average of correlation\n",
    "def calculate_ema(data, alpha):\n",
    "    ema = torch.zeros(len(data))\n",
    "    ema[0] = data[0]\n",
    "\n",
    "    for t in range(1, len(data)):\n",
    "        ema[t] = alpha * data[t] + (1 - alpha) * ema[t - 1]\n",
    "\n",
    "    return ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(torch.Size([8, 3, 80, 36, 64]),\\n torch.Size([8, 2, 80]),\\n torch.Size([8, 2, 80]),\\n torch.Size([8, 7440, 80]))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch.videos.shape, batch.pupil_center.shape, batch.behavior.shape, batch.responses.shape\n",
    "'''\n",
    "(torch.Size([8, 3, 80, 36, 64]),\n",
    " torch.Size([8, 2, 80]),\n",
    " torch.Size([8, 2, 80]),\n",
    " torch.Size([8, 7440, 80]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make experanto dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import experanto\n",
    "from experanto.dataloaders import get_multisession_dataloader\n",
    "from experanto.configs import DEFAULT_CONFIG as cfg\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screen 30 80\n",
      "responses 30 80\n",
      "eye_tracker 30 80\n",
      "treadmill 30 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/turishcheva/u14642/miniconda/envs/latent_model_finn/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pre_path_tr = '/mnt/vast-react/projects/neural_foundation_model/full_foundation_export_30hz_correct/'\n",
    "pre_path_test = '/mnt/vast-react/projects/neural_foundation_model/test_set_resampled/interpolate_with_hamming_30_30.0Hz/'\n",
    "\n",
    "train = [\n",
    "    'dynamic29156-11-10-Video-021a75e56847d574b9acbcc06c675055_30hz', \n",
    "    'dynamic29228-2-10-Video-021a75e56847d574b9acbcc06c675055_30hz', \n",
    "    'dynamic29234-6-9-Video-021a75e56847d574b9acbcc06c675055_30hz', \n",
    "    'dynamic29513-3-5-Video-021a75e56847d574b9acbcc06c675055_30hz', \n",
    "    'dynamic29514-2-9-Video-021a75e56847d574b9acbcc06c675055_30hz'\n",
    "]\n",
    "\n",
    "test_folder_scans = [\n",
    "    'dynamic26872-17-20-Video-021a75e56847d574b9acbcc06c675055_30hz',\n",
    "    'dynamic27204-5-13-Video-021a75e56847d574b9acbcc06c675055_30hz',\n",
    "    'dynamic29515-10-12-Video-021a75e56847d574b9acbcc06c675055_30hz',\n",
    "    'dynamic29623-4-9-Video-021a75e56847d574b9acbcc06c675055_30hz',\n",
    "    'dynamic29647-19-8-Video-021a75e56847d574b9acbcc06c675055_30hz',\n",
    "    'dynamic29712-5-9-Video-021a75e56847d574b9acbcc06c675055_30hz',\n",
    "    'dynamic29755-2-8-Video-021a75e56847d574b9acbcc06c675055_30hz'\n",
    "]\n",
    "\n",
    "full_paths = [f'{pre_path_tr}{t}/' for t in train] + [f'{pre_path_test}{t}/' for t in test_folder_scans]\n",
    "\n",
    "cfg['dataset']['modality_config']['responses']['sampling_rate'] = 30\n",
    "cfg['dataset']['modality_config']['responses']['chunk_size'] = 80\n",
    "\n",
    "cfg['dataset']['modality_config']['eye_tracker']['sampling_rate'] = 30\n",
    "cfg['dataset']['modality_config']['eye_tracker']['chunk_size'] = 80\n",
    "\n",
    "cfg['dataset']['modality_config']['treadmill']['sampling_rate'] = 30\n",
    "cfg['dataset']['modality_config']['treadmill']['chunk_size'] = 80\n",
    "\n",
    "cfg['dataset']['modality_config']['screen']['sampling_rate'] = 30\n",
    "cfg['dataset']['modality_config']['screen']['chunk_size'] =  80 \n",
    "\n",
    "for k in cfg.dataset.modality_config.keys():\n",
    "    print(k, cfg.dataset.modality_config[k].sampling_rate, cfg.dataset.modality_config[k].chunk_size)\n",
    "\n",
    "cfg['dataloader']['prefetch_factor'] = 2\n",
    "cfg['dataloader']['num_workers'] = 4\n",
    "cfg['dataloader']['shuffle'] = False\n",
    "cfg['dataloader']['pin_memory'] = False\n",
    "# cfg['dataset']['add_behavior_as_channels'] = True\n",
    "cfg['dataset']['modality_config']['screen']['transforms']['Resize']['size'] = [36, 64]\n",
    "\n",
    "\n",
    "train_dl = get_multisession_dataloader(full_paths, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(torch.Size([8, 3, 80, 36, 64]),\\n torch.Size([8, 2, 80]),\\n torch.Size([8, 2, 80]),\\n torch.Size([8, 7440, 80]))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch.videos.shape, batch.pupil_center.shape, batch.behavior.shape, batch.responses.shape\n",
    "'''\n",
    "(torch.Size([8, 3, 80, 36, 64]),\n",
    " torch.Size([8, 2, 80]),\n",
    " torch.Size([8, 2, 80]),\n",
    " torch.Size([8, 7440, 80]))\n",
    "'''\n",
    "# dilation, dilation derivative, center x , center y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beh = torch.cat([batch['eye_tracker'][:, :, :2].transpose(2, 1), batch['treadmill'][:, :, :2].transpose(2, 1)], axis=1)\n",
    "# video = batch['screen']\n",
    "\n",
    "# b_expanded = beh.unsqueeze(-1).unsqueeze(-1)  # or b[:, :, :, None, None]\n",
    "\n",
    "# # Now broadcast b to match the spatial dimensions [16, 3, 60, 144, 256]\n",
    "# beh_tiled = b_expanded.expand(-1, -1, -1, video.shape[3], video.shape[4])\n",
    "\n",
    "# # Concatenate along dim=1 to get [16, 4, 60, 144, 256]\n",
    "# video = torch.cat([video, beh_tiled], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beh = torch.cat([batch['eye_tracker'][:, :, :2].transpose(2, 1), batch['treadmill'][:, :, :2].transpose(2, 1)], axis=1)\n",
    "# video = batch['screen']\n",
    "# b_expanded = beh.unsqueeze(-1).unsqueeze(-1)  # or b[:, :, :, None, None]\n",
    "# # Now broadcast b to match the spatial dimensions [16, 3, 60, 144, 256]\n",
    "# beh_tiled = b_expanded.expand(-1, -1, -1, video.shape[3], video.shape[4])\n",
    "# # Concatenate along dim=1 to get [16, 4, 60, 144, 256]\n",
    "# video = torch.cat([video, beh_tiled], dim=1)\n",
    "\n",
    "# resp = batch['responses'].transpose(2, 1)\n",
    "# [video, resp,  ]\n",
    "# batch_kwargs = {\n",
    "#     'videos': video,\n",
    "#     'pupil_center': batch['eye_tracker'][:, :, 2:].transpose(2, 1),\n",
    "#     'responses': resp\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'global_sampling_rate': None, 'global_chunk_size': None, 'add_behavior_as_channels': False, 'replace_nans_with_means': False, 'cache_data': False, 'out_keys': ['screen', 'responses', 'eye_tracker', 'treadmill', 'timestamps'], 'normalize_timestamps': True, 'modality_config': {'screen': {'keep_nans': False, 'sampling_rate': 30, 'chunk_size': 60, 'valid_condition': {'tier': 'train'}, 'offset': 0, 'sample_stride': 1, 'include_blanks': True, 'transforms': {'normalization': 'normalize', 'Resize': {'_target_': 'torchvision.transforms.v2.Resize', 'size': [144, 256]}}, 'interpolation': {'rescale': True, 'rescale_size': [144, 256]}}, 'responses': {'keep_nans': False, 'sampling_rate': 30, 'chunk_size': 60, 'offset': 0.0, 'transforms': {'normalization': 'standardize'}, 'interpolation': {'interpolation_mode': 'nearest_neighbor'}, 'filters': {'nan_filter': {'__target__': 'experanto.filters.common_filters.nan_filter', '__partial__': True, 'vicinity': 0.05}}}, 'eye_tracker': {'keep_nans': False, 'sampling_rate': 30, 'chunk_size': 60, 'offset': 0, 'transforms': {'normalization': 'normalize'}, 'interpolation': {'interpolation_mode': 'nearest_neighbor'}, 'filters': {'nan_filter': {'__target__': 'experanto.filters.common_filters.nan_filter', '__partial__': True, 'vicinity': 0.05}}}, 'treadmill': {'keep_nans': False, 'sampling_rate': 30, 'chunk_size': 60, 'offset': 0, 'transforms': {'normalization': 'normalize'}, 'interpolation': {'interpolation_mode': 'nearest_neighbor'}, 'filters': {'nan_filter': {'__target__': 'experanto.filters.common_filters.nan_filter', '__partial__': True, 'vicinity': 0.05}}}}}, 'dataloader': {'batch_size': 16, 'shuffle': False, 'num_workers': 4, 'pin_memory': False, 'drop_last': True, 'prefetch_factor': 2}}\n"
     ]
    }
   ],
   "source": [
    "# print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activity_dict = {}\n",
    "n_neurons_dict = {}\n",
    "data_keys = list(train_dl.loaders.keys())\n",
    "for k in data_keys:\n",
    "    batch = next(iter(train_dl.loaders[k]))\n",
    "    n_neurons_dict[k] = batch['responses'].shape[-1]\n",
    "    mean_activity_dict[k] = batch['responses'].reshape(-1, n_neurons_dict[k]).mean(axis=0)\n",
    "\n",
    "batch_size = batch['responses'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorised_3D_core_dict = dict(\n",
    "    input_channels=4,\n",
    "    hidden_channels=[32, 64, 128],\n",
    "    spatial_input_kernel=(11, 11),\n",
    "    temporal_input_kernel=11,\n",
    "    spatial_hidden_kernel=(5, 5),\n",
    "    temporal_hidden_kernel=5,\n",
    "    stride=1,\n",
    "    layers=3,\n",
    "    gamma_input_spatial=10,\n",
    "    gamma_input_temporal=0.01,\n",
    "    bias=True,\n",
    "    hidden_nonlinearities=\"elu\",\n",
    "    x_shift=0,\n",
    "    y_shift=0,\n",
    "    batch_norm=True,\n",
    "    laplace_padding=None,\n",
    "    input_regularizer=\"LaplaceL2norm\",\n",
    "    padding=False,\n",
    "    final_nonlin=True,\n",
    "    momentum=0.7,\n",
    ")\n",
    "shifter_dict = dict(\n",
    "    gamma_shifter=0,\n",
    "    shift_layers=3,\n",
    "    input_channels_shifter=2,\n",
    "    hidden_channels_shifter=5,\n",
    ")\n",
    "\n",
    "\n",
    "readout_dict = dict(\n",
    "    bias=False,\n",
    "    init_mu_range=0.2,\n",
    "    init_sigma=1.0,\n",
    "    gamma_readout=0.0,\n",
    "    gauss_type=\"full\",\n",
    "    # grid_mean_predictor={\n",
    "    #     \"type\": \"cortex\",\n",
    "    #     \"input_dimensions\": 2,\n",
    "    #     \"hidden_layers\": 1,\n",
    "    #     \"hidden_features\": 30,\n",
    "    #     \"final_tanh\": True,\n",
    "    # },\n",
    "    grid_mean_predictor = None,\n",
    "    share_features=False,\n",
    "    share_grid=False,\n",
    "    shared_match_ids=None,\n",
    "    gamma_grid_dispersion=0.0,\n",
    "    zig=True,\n",
    "    out_channels=2,\n",
    "    kernel_size=(11, 5),\n",
    "    batch_size=cfg['dataloader']['batch_size'],\n",
    "    # conv_out = conv_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/vast-nhr/home/turishcheva/u14642/finn_model_january25/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The batch_norm is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/mnt/vast-nhr/home/turishcheva/u14642/finn_model_january25/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The bias is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/mnt/vast-nhr/home/turishcheva/u14642/finn_model_january25/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The batch_norm_scale is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/mnt/vast-nhr/home/turishcheva/u14642/finn_model_january25/neuralpredictors/neuralpredictors/layers/readouts/base.py:74: UserWarning: Use of 'gamma_readout' is deprecated. Use 'feature_reg_weight' instead. If 'feature_reg_weight' is defined, 'gamma_readout' is ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "factorised_3d_model = make_video_model(\n",
    "    None,\n",
    "    seed,\n",
    "    core_dict=factorised_3D_core_dict,\n",
    "    core_type=\"3D_factorised\",\n",
    "    readout_dict=readout_dict.copy(),\n",
    "    readout_type=\"gaussian\",\n",
    "    use_gru=False,\n",
    "    gru_dict=None,\n",
    "    use_shifter=True,  # set to True if behavior is included\n",
    "    shifter_dict=shifter_dict,\n",
    "    shifter_type=\"MLP\",\n",
    "    deeplake_ds=False,\n",
    "    n_neurons_dict=n_neurons_dict,\n",
    "    mean_activity_dict=mean_activity_dict,\n",
    "    experanto=True,\n",
    "    readout_dim=factorised_3D_core_dict['hidden_channels'][-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoFiringRateEncoder(\n",
       "  (core): Factorized3dCore(\n",
       "    (_input_weight_regularizer): LaplaceL2norm(\n",
       "      (laplace): Laplace()\n",
       "    )\n",
       "    (temporal_regularizer): DepthLaplaceL21d(\n",
       "      (laplace): Laplace1d()\n",
       "    )\n",
       "    (features): Sequential(\n",
       "      (layer0): Sequential(\n",
       "        (conv_spatial): Conv3d(4, 32, kernel_size=(1, 11, 11), stride=(1, 1, 1))\n",
       "        (conv_temporal): Conv3d(32, 32, kernel_size=(11, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(32, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "      (layer1): Sequential(\n",
       "        (conv_spatial_1): Conv3d(32, 64, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "        (conv_temporal_1): Conv3d(64, 64, kernel_size=(5, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(64, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (conv_spatial_2): Conv3d(64, 128, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "        (conv_temporal_2): Conv3d(128, 128, kernel_size=(5, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(128, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "    )\n",
       "  ) [Factorized3dCore regularizers: gamma_input_spatial = 10|gamma_input_temporal = 0.01]\n",
       "  \n",
       "  (readout): MultipleFullGaussian2d(\n",
       "    (29156-11-10): full FullGaussian2d (128 x 18 x 46 -> 7440)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29228-2-10): full FullGaussian2d (128 x 18 x 46 -> 7928)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29234-6-9): full FullGaussian2d (128 x 18 x 46 -> 8285)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29513-3-5): full FullGaussian2d (128 x 18 x 46 -> 7671)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29514-2-9): full FullGaussian2d (128 x 18 x 46 -> 7495)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (26872-17-20): full FullGaussian2d (128 x 18 x 46 -> 7776)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (27204-5-13): full FullGaussian2d (128 x 18 x 46 -> 7538)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29515-10-12): full FullGaussian2d (128 x 18 x 46 -> 7863)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29623-4-9): full FullGaussian2d (128 x 18 x 46 -> 7908)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29647-19-8): full FullGaussian2d (128 x 18 x 46 -> 8202)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29712-5-9): full FullGaussian2d (128 x 18 x 46 -> 7939)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29755-2-8): full FullGaussian2d (128 x 18 x 46 -> 8122)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "  )\n",
       "  (shifter): MLPShifter(\n",
       "    (29156-11-10): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29228-2-10): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29234-6-9): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29513-3-5): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29514-2-9): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (26872-17-20): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (27204-5-13): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29515-10-12): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29623-4-9): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29647-19-8): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29712-5-9): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29755-2-8): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "  )\n",
       "  (nonlinearity_fn): ELU(alpha=1.0)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorised_3d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precompute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import root\n",
    "# from scipy.stats import gamma\n",
    "# import gc\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.dataset.modality_config['screen']['sample_stride'] = cfg['dataset']['modality_config']['screen']['chunk_size']\n",
    "# cfg.dataset.modality_config['screen']['sample_stride']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor m in full_paths:\\n    train_dl = get_multisession_dataloader([m], cfg)\\n    mouse_name = m.split(\\'dynamic\\')[-1].split(\\'-Video\\')[0]\\n    num_neurons = train_dl.loaders[mouse_name].dataset.__getitem__(0)[\\'responses\\'].cpu().numpy().shape[-1]\\n    step = 16 * 60\\n    concatenated_array = np.zeros((num_neurons, len(train_dl) * step), dtype=\\'float32\\')\\n    for idx, batch in enumerate(tqdm(train_dl)):\\n        batch = batch[1]\\n        resp_b = batch[\\'responses\\'].cpu().numpy()\\n        resp_b = resp_b.reshape(-1, *resp_b.shape[2:]).T\\n        # # print(resp_b.shape)\\n        # resp_b = np.where(resp_b < 0.005, np.nan, resp_b)\\n        # # print(resp_b.shape)\\n        # resp_b = np.minimum(resp_b, 80)\\n        # # print(resp_b.shape)\\n        # # resp_b = resp_b[1, ~np.isnan(resp_b[0, :])]\\n        # # print(resp_b.shape)\\n        concatenated_array[:, step*idx : step*(idx+1)] = resp_b\\n        \\n    print(concatenated_array.shape)\\n\\n    # Filter out values smaller than 0.005, this is roughly the threshold of the ZIG distribution. \\n    # Only values bigger than this threshold are considered in the gamma distribution\\n    filtered_array = np.where(\\n        concatenated_array <= 0.005, np.nan, concatenated_array\\n    )\\n    filtered_array = np.minimum(filtered_array, 80)  # cut outliers off\\n    first_row_valid_values = filtered_array[\\n        1, ~np.isnan(filtered_array[0, :])\\n    ]\\n    k_params = []\\n\\n    for i in tqdm(range(filtered_array.shape[0])):\\n        neuron_valid_values = filtered_array[\\n            i, ~np.isnan(filtered_array[i, :])\\n        ]\\n        # Fix the loc parameter to 0.05\\n        params = gamma.fit(neuron_valid_values, floc=0.005)\\n\\n        # Get the shape and scale parameters\\n        k, loc, theta = params\\n        k_params.append(k)\\n\\n    # Get the shape, location, and scale parameters\\n    k, loc, theta = params\\n    x = np.linspace(\\n        min(first_row_valid_values), max(first_row_valid_values), 1000\\n    )\\n    fitted_gamma = gamma.pdf(x, k, loc, theta)\\n\\n    # Calculate mean and variance along the first dimension (axis 1) ignoring nan values\\n    mean_array = np.nanmean(filtered_array, axis=1)\\n    variance_array = np.nanvar(filtered_array, axis=1)\\n    \\n    print(\\'before saving\\')\\n    os.makedirs(f\\'/scratch-grete/projects/nim00012/experanto_stats_finn_model/{m.split(\"/\")[-2]}\\', exist_ok=True) \\n    np.save(f\\'/scratch-grete/projects/nim00012/experanto_stats_finn_model/{m.split(\"/\")[-2]}/new_mean.npy\\', mean_array)\\n    np.save(f\\'/scratch-grete/projects/nim00012/experanto_stats_finn_model/{m.split(\"/\")[-2]}/new_variance.npy\\', variance_array)\\n    np.save(f\\'/scratch-grete/projects/nim00012/experanto_stats_finn_model/{m.split(\"/\")[-2]}/k_fitted.npy\\', np.array(k_params))\\n    del concatenated_array\\n    torch.cuda.empty_cache()  # If using GPU tensors\\n    gc.collect()              # Force Python garbage collection\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for m in full_paths:\n",
    "    train_dl = get_multisession_dataloader([m], cfg)\n",
    "    mouse_name = m.split('dynamic')[-1].split('-Video')[0]\n",
    "    num_neurons = train_dl.loaders[mouse_name].dataset.__getitem__(0)['responses'].cpu().numpy().shape[-1]\n",
    "    step = 16 * 60\n",
    "    concatenated_array = np.zeros((num_neurons, len(train_dl) * step), dtype='float32')\n",
    "    for idx, batch in enumerate(tqdm(train_dl)):\n",
    "        batch = batch[1]\n",
    "        resp_b = batch['responses'].cpu().numpy()\n",
    "        resp_b = resp_b.reshape(-1, *resp_b.shape[2:]).T\n",
    "        # # print(resp_b.shape)\n",
    "        # resp_b = np.where(resp_b < 0.005, np.nan, resp_b)\n",
    "        # # print(resp_b.shape)\n",
    "        # resp_b = np.minimum(resp_b, 80)\n",
    "        # # print(resp_b.shape)\n",
    "        # # resp_b = resp_b[1, ~np.isnan(resp_b[0, :])]\n",
    "        # # print(resp_b.shape)\n",
    "        concatenated_array[:, step*idx : step*(idx+1)] = resp_b\n",
    "        \n",
    "    print(concatenated_array.shape)\n",
    "\n",
    "    # Filter out values smaller than 0.005, this is roughly the threshold of the ZIG distribution. \n",
    "    # Only values bigger than this threshold are considered in the gamma distribution\n",
    "    filtered_array = np.where(\n",
    "        concatenated_array <= 0.005, np.nan, concatenated_array\n",
    "    )\n",
    "    filtered_array = np.minimum(filtered_array, 80)  # cut outliers off\n",
    "    first_row_valid_values = filtered_array[\n",
    "        1, ~np.isnan(filtered_array[0, :])\n",
    "    ]\n",
    "    k_params = []\n",
    "\n",
    "    for i in tqdm(range(filtered_array.shape[0])):\n",
    "        neuron_valid_values = filtered_array[\n",
    "            i, ~np.isnan(filtered_array[i, :])\n",
    "        ]\n",
    "        # Fix the loc parameter to 0.05\n",
    "        params = gamma.fit(neuron_valid_values, floc=0.005)\n",
    "\n",
    "        # Get the shape and scale parameters\n",
    "        k, loc, theta = params\n",
    "        k_params.append(k)\n",
    "\n",
    "    # Get the shape, location, and scale parameters\n",
    "    k, loc, theta = params\n",
    "    x = np.linspace(\n",
    "        min(first_row_valid_values), max(first_row_valid_values), 1000\n",
    "    )\n",
    "    fitted_gamma = gamma.pdf(x, k, loc, theta)\n",
    "\n",
    "    # Calculate mean and variance along the first dimension (axis 1) ignoring nan values\n",
    "    mean_array = np.nanmean(filtered_array, axis=1)\n",
    "    variance_array = np.nanvar(filtered_array, axis=1)\n",
    "    \n",
    "    print('before saving')\n",
    "    os.makedirs(f'/scratch-grete/projects/nim00012/experanto_stats_finn_model/{m.split(\"/\")[-2]}', exist_ok=True) \n",
    "    np.save(f'/scratch-grete/projects/nim00012/experanto_stats_finn_model/{m.split(\"/\")[-2]}/new_mean.npy', mean_array)\n",
    "    np.save(f'/scratch-grete/projects/nim00012/experanto_stats_finn_model/{m.split(\"/\")[-2]}/new_variance.npy', variance_array)\n",
    "    np.save(f'/scratch-grete/projects/nim00012/experanto_stats_finn_model/{m.split(\"/\")[-2]}/k_fitted.npy', np.array(k_params))\n",
    "    del concatenated_array\n",
    "    torch.cuda.empty_cache()  # If using GPU tensors\n",
    "    gc.collect()              # Force Python garbage collection\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of statistics precompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/scratch-grete/projects/nim00012/experanto_stats_finn_model/\"\n",
    "mean_variance_dict = load_mean_variance(base_dir, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_variance_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dynamic29712-5-9-Video-021a75e56847d574b9acbcc06c675055_30hz_mean'.replace('dynamic', '').replace('-Video-021a75e56847d574b9acbcc06c675055_30hz', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_variance_dict_new = {}\n",
    "for k, v in mean_variance_dict.items():\n",
    "    mean_variance_dict_new[k.replace('dynamic', '').replace('-Video-021a75e56847d574b9acbcc06c675055_30hz', '')] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'29712-5-9_mean': tensor([0.7078, 0.8658, 0.9356,  ..., 0.8265, 0.9993, 0.8470], device='cuda:0'),\n",
       " '29712-5-9_variance': tensor([3.8925, 2.0632, 2.1666,  ..., 1.5529, 1.5811, 1.6078], device='cuda:0'),\n",
       " '29712-5-9fitted_k': tensor([0.4327, 0.5413, 0.5173,  ..., 0.5679, 0.5819, 0.5654], device='cuda:0'),\n",
       " '29755-2-8_mean': tensor([0.8733, 0.8881, 0.9002,  ..., 0.7974, 0.8424, 0.7204], device='cuda:0'),\n",
       " '29755-2-8_variance': tensor([1.6552, 2.1725, 1.9426,  ..., 1.8062, 2.9628, 2.5200], device='cuda:0'),\n",
       " '29755-2-8fitted_k': tensor([0.5569, 0.5238, 0.5345,  ..., 0.5534, 0.4401, 0.4859], device='cuda:0'),\n",
       " '29228-2-10_mean': tensor([0.5274, 0.4098, 0.5304,  ..., 0.9011, 0.7784, 0.8361], device='cuda:0'),\n",
       " '29228-2-10_variance': tensor([1.4311, 1.5626, 1.6986,  ..., 2.0438, 1.6766, 1.3538], device='cuda:0'),\n",
       " '29228-2-10fitted_k': tensor([0.4827, 0.4904, 0.4743,  ..., 0.4863, 0.5019, 0.5391], device='cuda:0'),\n",
       " '29647-19-8_mean': tensor([0.8073, 0.8573, 0.9913,  ..., 0.8002, 0.8457, 1.0069], device='cuda:0'),\n",
       " '29647-19-8_variance': tensor([2.0719, 2.8757, 1.9392,  ..., 3.2802, 2.3179, 3.0609], device='cuda:0'),\n",
       " '29647-19-8fitted_k': tensor([0.5066, 0.4631, 0.5541,  ..., 0.4538, 0.5005, 0.4764], device='cuda:0'),\n",
       " '29515-10-12_mean': tensor([0.9624, 0.8379, 0.7703,  ..., 0.8926, 0.7317, 0.8389], device='cuda:0'),\n",
       " '29515-10-12_variance': tensor([2.4180, 1.8456, 2.8111,  ..., 2.4229, 2.9148, 2.2786], device='cuda:0'),\n",
       " '29515-10-12fitted_k': tensor([0.5256, 0.5388, 0.4703,  ..., 0.4969, 0.4517, 0.5227], device='cuda:0'),\n",
       " '29513-3-5_mean': tensor([0.6642, 0.6405, 0.5895,  ..., 0.5788, 0.6925, 0.4723], device='cuda:0'),\n",
       " '29513-3-5_variance': tensor([1.1640, 0.9579, 1.0217,  ..., 1.8103, 2.0936, 1.2978], device='cuda:0'),\n",
       " '29513-3-5fitted_k': tensor([0.5279, 0.5724, 0.5602,  ..., 0.4336, 0.4550, 0.5289], device='cuda:0'),\n",
       " '29234-6-9_mean': tensor([0.6051, 0.6795, 0.5169,  ..., 0.5536, 0.5684, 0.4421], device='cuda:0'),\n",
       " '29234-6-9_variance': tensor([0.7714, 0.5781, 1.0023,  ..., 1.6348, 0.9153, 1.3395], device='cuda:0'),\n",
       " '29234-6-9fitted_k': tensor([0.6314, 0.6569, 0.5425,  ..., 0.4709, 0.5613, 0.4929], device='cuda:0'),\n",
       " '29156-11-10_mean': tensor([0.6843, 0.7068, 0.7955,  ..., 0.6693, 0.7351, 0.7038], device='cuda:0'),\n",
       " '29156-11-10_variance': tensor([0.6687, 1.1141, 1.2980,  ..., 1.5758, 0.8167, 1.5499], device='cuda:0'),\n",
       " '29156-11-10fitted_k': tensor([0.6226, 0.5423, 0.5313,  ..., 0.4830, 0.5799, 0.4989], device='cuda:0'),\n",
       " '29623-4-9_mean': tensor([0.9175, 1.0571, 1.0372,  ..., 0.7661, 0.5891, 0.9131], device='cuda:0'),\n",
       " '29623-4-9_variance': tensor([2.3918, 2.3800, 2.8715,  ..., 2.9203, 2.5263, 1.9400], device='cuda:0'),\n",
       " '29623-4-9fitted_k': tensor([0.5108, 0.5169, 0.4961,  ..., 0.4476, 0.4780, 0.5379], device='cuda:0'),\n",
       " '29514-2-9_mean': tensor([0.6406, 0.5000, 0.6711,  ..., 0.6998, 0.6500, 0.6713], device='cuda:0'),\n",
       " '29514-2-9_variance': tensor([1.4842, 1.3879, 2.0454,  ..., 1.8047, 1.4510, 1.4917], device='cuda:0'),\n",
       " '29514-2-9fitted_k': tensor([0.5113, 0.4885, 0.4422,  ..., 0.4210, 0.4903, 0.5216], device='cuda:0'),\n",
       " '26872-17-20_mean': tensor([0.8428, 0.8900, 0.8032,  ..., 0.7886, 0.8481, 0.9460], device='cuda:0'),\n",
       " '26872-17-20_variance': tensor([1.9649, 2.2564, 2.1135,  ..., 2.6544, 3.1470, 4.6392], device='cuda:0'),\n",
       " '26872-17-20fitted_k': tensor([0.5251, 0.5081, 0.5057,  ..., 0.4936, 0.4711, 0.4281], device='cuda:0'),\n",
       " '27204-5-13_mean': tensor([0.7662, 0.8541, 0.8583,  ..., 1.2511, 0.8109, 0.7200], device='cuda:0'),\n",
       " '27204-5-13_variance': tensor([2.2645, 3.1320, 3.5641,  ..., 3.7546, 4.4903, 2.1612], device='cuda:0'),\n",
       " '27204-5-13fitted_k': tensor([0.4803, 0.4215, 0.4623,  ..., 0.5056, 0.3843, 0.4869], device='cuda:0')}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_variance_dict_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7939]), torch.Size([7939]), torch.Size([7939]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_variance_dict_new['29712-5-9_mean'].shape, mean_variance_dict_new['29712-5-9_variance'].shape, mean_variance_dict_new['29712-5-9fitted_k'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'29156-11-10': 7440,\n",
       " '29228-2-10': 7928,\n",
       " '29234-6-9': 8285,\n",
       " '29513-3-5': 7671,\n",
       " '29514-2-9': 7495,\n",
       " '26872-17-20': 7776,\n",
       " '27204-5-13': 7538,\n",
       " '29515-10-12': 7863,\n",
       " '29623-4-9': 7908,\n",
       " '29647-19-8': 8202,\n",
       " '29712-5-9': 7939,\n",
       " '29755-2-8': 8122}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neurons_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine maximal number of neurons\n",
    "max_neurons = max(list(n_neurons_dict.values()))\n",
    "\n",
    "output_dim = 12\n",
    "dropout = \"across_time\"\n",
    "dropout_prob = 0.5\n",
    "encoder_dict = {}\n",
    "encoder_dict[\"input_dim\"] = max_neurons\n",
    "encoder_dict[\"hidden_dim\"] = 42  # 42\n",
    "encoder_dict[\"hidden_gru\"] = 20  # 20\n",
    "encoder_dict[\"output_dim\"] = output_dim\n",
    "encoder_dict[\"hidden_layers\"] = 1\n",
    "encoder_dict[\"n_samples\"] = 70\n",
    "encoder_dict[\"mice_dim\"] = 0\n",
    "encoder_dict[\"use_cnn\"] = False\n",
    "encoder_dict[\"residual\"] = False\n",
    "encoder_dict[\"kernel_size\"] = [11, 5, 5]\n",
    "encoder_dict[\"channel_size\"] = [32, 32, 20]\n",
    "encoder_dict[\"use_resnet\"] = False\n",
    "encoder_dict[\"pretrained\"] = True\n",
    "\n",
    "decoder_dict = {}\n",
    "decoder_dict[\"hidden_dim\"] = output_dim\n",
    "decoder_dict[\"hidden_layers\"] = 1\n",
    "decoder_dict[\"use_cnn\"] = False\n",
    "decoder_dict[\"kernel_size\"] = [5, 11]\n",
    "decoder_dict[\"channel_size\"] = [12, 12]\n",
    "\n",
    "# TODO latent should be TRUE after ZIG pretraining\n",
    "latent = False\n",
    "zig_model = ZIGEncoder(\n",
    "    core=factorised_3d_model.core,\n",
    "    readout=factorised_3d_model.readout,\n",
    "    shifter = factorised_3d_model.shifter,\n",
    "    # shifter=None,\n",
    "    k_image_dependent=False,\n",
    "    loc_image_dependent=False,\n",
    "    mle_fitting=mean_variance_dict_new,\n",
    "    latent=latent,\n",
    "    encoder=encoder_dict,\n",
    "    # decoder = decoder_dict,\n",
    "    norm_layer=\"layer_flex\",\n",
    "    non_linearity=True,\n",
    "    dropout=dropout,\n",
    "    dropout_prob=dropout_prob,\n",
    "    future_prediction=False,\n",
    "    flow=False,\n",
    "    position_features = None,\n",
    "    behavior_in_encoder = None\n",
    ")\n",
    "if not latent:\n",
    "    zig_model.flow = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets adjust the train loop for experanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_trainer(\n",
    "    model,\n",
    "    dataloaders,\n",
    "    seed,\n",
    "    avg_loss=False,\n",
    "    scale_loss=True,\n",
    "    loss_function=\"PoissonLoss\",\n",
    "    stop_function=\"get_correlations\",\n",
    "    loss_accum_batch_n=None,\n",
    "    device=\"cuda\",\n",
    "    verbose=True,\n",
    "    interval=1,\n",
    "    patience=5,\n",
    "    epoch=0,\n",
    "    lr_init=0.005,\n",
    "    max_iter=200,\n",
    "    maximize=True,\n",
    "    tolerance=1e-6,\n",
    "    restore_best=True,\n",
    "    lr_decay_steps=3,\n",
    "    lr_decay_factor=0.3,\n",
    "    min_lr=0.0001,\n",
    "    cb=None,\n",
    "    detach_core=False,\n",
    "    use_wandb=True,\n",
    "    wandb_project='finn_mode_with_experanto',\n",
    "    wandb_entity=\"ecker-lab\",\n",
    "    wandb_name=None,\n",
    "    wandb_model_config=None,\n",
    "    wandb_dataset_config=None,\n",
    "    save_checkpoints=True,\n",
    "    checkpoint_save_path=\"local/\",\n",
    "    chpt_save_step=15,\n",
    "    k_reg=False,\n",
    "    ema_span=0.3,  # ema for validation correlation\n",
    "    scheduler_patience=6,  # patience for decaying learning rate\n",
    "    latent=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        model: model to be trained\n",
    "        dataloaders: dataloaders containing the data to train the model with\n",
    "        seed: random seed\n",
    "        avg_loss: whether to average (or sum) the loss over a batch\n",
    "        scale_loss: whether to scale the loss according to the size of the dataset\n",
    "        loss_function: loss function to use\n",
    "        stop_function: the function (metric) that is used to determine the end of the training in early stopping\n",
    "        loss_accum_batch_n: number of batches to accumulate the loss over\n",
    "        device: device to run the training on\n",
    "        verbose: whether to print out a message for each optimizer step\n",
    "        interval: interval at which objective is evaluated to consider early stopping\n",
    "        patience: number of times the objective is allowed to not become better before the iterator terminates\n",
    "        epoch: starting epoch\n",
    "        lr_init: initial learning rate\n",
    "        max_iter: maximum number of training iterations\n",
    "        maximize: whether to maximize or minimize the objective function\n",
    "        tolerance: tolerance for early stopping\n",
    "        restore_best: whether to restore the model to the best state after early stopping\n",
    "        lr_decay_steps: how many times to decay the learning rate after no improvement\n",
    "        lr_decay_factor: factor to decay the learning rate with\n",
    "        min_lr: minimum learning rate\n",
    "        warmup_steps: number of batch steps for linear lr warump\n",
    "        T_max: epoch periodicity of cosine annealing schedluer\n",
    "        cb: whether to execute callback function\n",
    "        zig : True if ZIG encoder is used as model\n",
    "        k_reg: is a dictonary containg the fitted k_values for each mice, applies regularization to size of shape parameter k of gamma distribution if k_reg is not None but a dictionary,\n",
    "        ema-Span: alpha factor of exponential moving avaerage of validation correlation\n",
    "        **kwargs:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    print(loss_function)\n",
    "\n",
    "    def full_objective(model, dataloader, data_key, *args, k_regu=k_reg, **kwargs):\n",
    "        loss_scale = (\n",
    "            np.sqrt(len(dataloader[data_key].dataset) / args[0].shape[0])\n",
    "            if scale_loss\n",
    "            else 1.0\n",
    "        )\n",
    "        if not isinstance(model.core.regularizer(), tuple):\n",
    "            regularizers = int(\n",
    "                not detach_core\n",
    "            ) * model.core.regularizer() + model.readout.regularizer(data_key)\n",
    "        else:\n",
    "            regularizers = int(not detach_core) * sum(\n",
    "                model.core.regularizer()\n",
    "            ) + model.readout.regularizer(data_key)\n",
    "        if loss_function == \"ZIGLoss\":\n",
    "            # one entry in a tuple corresponds to one paramter of ZIG\n",
    "            # the output is (theta,k,loc,q)\n",
    "            positions = None\n",
    "            # args[0][0:1] removes behavior from the video input data.\n",
    "            model_output = model(\n",
    "                args[0].to(device),\n",
    "                data_key=data_key,\n",
    "                out_predicts=False,\n",
    "                train=True,\n",
    "                positions=positions,\n",
    "                **kwargs,\n",
    "            )\n",
    "            theta = model_output[0]\n",
    "            k = model_output[1]\n",
    "            loc = model_output[2]\n",
    "            q = model_output[3]\n",
    "            time_left = k.shape[1]\n",
    "\n",
    "            original_data = args[1].transpose(2, 1)[:, -time_left:, :].to(device)\n",
    "\n",
    "            # create zero, non zero masks\n",
    "            comparison_result = original_data >= loc\n",
    "            nonzero_mask = comparison_result.int()\n",
    "\n",
    "            comparison_result = original_data <= loc\n",
    "            zero_mask = comparison_result.int()\n",
    "\n",
    "            if k_regu:\n",
    "                k_fitted = k_regu[data_key + \"fitted_k\"]\n",
    "                # k is constant over time and has shape (batch_size,time,num_neurons)\n",
    "                k_output = k[0, 0, :]\n",
    "                # punish values of k that are far away from the fitted k value, scale the regularization to the size of zig loss\n",
    "                k_regularized = ((k_fitted - k_output) ** 2).mean() * 7 * 10**7\n",
    "\n",
    "            else:\n",
    "                k_regularized = 0\n",
    "\n",
    "            if (\n",
    "                len(model_output) > 4\n",
    "            ):  # that is the case only for the latent space model\n",
    "                k = k.unsqueeze(-1)\n",
    "                loc = loc.unsqueeze(-1)\n",
    "                zero_mask = zero_mask.unsqueeze(-1)\n",
    "                nonzero_mask = nonzero_mask.unsqueeze(-1)\n",
    "                original_data = original_data.unsqueeze(-1)\n",
    "                means = model_output[4]\n",
    "                sigma_squared = model_output[5]\n",
    "                n_samples = model_output[7]\n",
    "\n",
    "                sigma = torch.sqrt(sigma_squared)\n",
    "                # Mask neurons, which were given in Encoder\n",
    "                neuron_mask = model_output[8].to(means.device)\n",
    "                neuron_mask = neuron_mask.unsqueeze(-1).repeat(1, 1, 1, n_samples)\n",
    "                zig_loss = (\n",
    "                    -1\n",
    "                    * loss_scale\n",
    "                    * (\n",
    "                        criterion(\n",
    "                            theta,\n",
    "                            k,\n",
    "                            loc=loc,\n",
    "                            q=q,\n",
    "                            target=original_data,\n",
    "                            zero_mask=zero_mask,\n",
    "                            nonzero_mask=nonzero_mask,\n",
    "                        )[0]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                zig_loss.masked_fill_(~neuron_mask, 0)\n",
    "                zig_loss = zig_loss.sum() + regularizers\n",
    "\n",
    "                zig_loss = zig_loss / (\n",
    "                    n_samples\n",
    "                )  # zigloss is in that case an MC approximate for the mean of log p(y|x,z)\n",
    "                # calculate KL divergence between Gaussian prior and approximate posterior\n",
    "\n",
    "                kl_divergence = kl_divergence_gaussian(\n",
    "                    means, sigma\n",
    "                )  # * q.shape[2] #kl_divergence is constant across neuron dimension since latent is the same for all neurons\n",
    "\n",
    "                differences = means[:, 1:] - means[:, :-1]\n",
    "                neighbor_loss = torch.norm(differences, p=2, dim=2).sum()\n",
    "\n",
    "                # average loss over batch_size and time\n",
    "                zig_loss = (\n",
    "                    1\n",
    "                    / (means.shape[0] * means.shape[1])\n",
    "                    * (zig_loss + 5 * kl_divergence)\n",
    "                )  # loss is ElBO = p(y|x,z) + KL(q(z|x),p(z)), log_det=0 if no flow is applied\n",
    "\n",
    "            else:\n",
    "                zig_loss = (\n",
    "                    -1\n",
    "                    * loss_scale\n",
    "                    * criterion(\n",
    "                        theta,\n",
    "                        k,\n",
    "                        loc=loc,\n",
    "                        q=q,\n",
    "                        target=original_data,\n",
    "                        zero_mask=zero_mask,\n",
    "                        nonzero_mask=nonzero_mask,\n",
    "                    )[0].sum()\n",
    "                    + regularizers\n",
    "                    + k_regularized\n",
    "                )\n",
    "            # only zig loss\n",
    "            if len(model_output) > 4:\n",
    "                return zig_loss, kl_divergence\n",
    "            else:\n",
    "                return zig_loss\n",
    "            \"\"\"\n",
    "            return (\n",
    "                -1*loss_scale\n",
    "                * criterion(theta, k,\n",
    "                            loc=loc, \n",
    "                            q=q, \n",
    "                            target=original_data, \n",
    "                            zero_mask=zero_mask, \n",
    "                            nonzero_mask=nonzero_mask)[0].sum()\n",
    "                + regularizers\n",
    "            )\n",
    "            \"\"\"\n",
    "        else:\n",
    "            model_output = model(args[0].to(device), data_key=data_key, **kwargs)\n",
    "            time_left = model_output.shape[1]\n",
    "\n",
    "            original_data = args[1].transpose(2, 1)[:, -time_left:, :].to(device)\n",
    "\n",
    "            return (\n",
    "                loss_scale\n",
    "                * criterion(\n",
    "                    model_output,\n",
    "                    original_data,\n",
    "                )\n",
    "                + regularizers\n",
    "            )\n",
    "\n",
    "    ##### Model training ####################################################################################################\n",
    "    model.to(device)\n",
    "    set_random_seed(seed)\n",
    "    model.train()\n",
    "    if loss_function == \"ZIGLoss\":\n",
    "        zig_loss_instance = zero_inflated_losses.ZIGLoss()\n",
    "        criterion = zig_loss_instance.get_slab_logl\n",
    "    else:\n",
    "        criterion = getattr(modules, loss_function)(avg=avg_loss)\n",
    "    stop_closure = partial(\n",
    "        getattr(scores, stop_function),\n",
    "        dataloaders=dataloaders[\"oracle\"],\n",
    "        device=device,\n",
    "        per_neuron=False,\n",
    "        avg=True,\n",
    "        flow=model.flow,\n",
    "        cell_coordinates=None,\n",
    "    )\n",
    "\n",
    "    n_iterations = len(dataloaders[\"train\"])\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr_init)\n",
    "    # Define the optimizer to only include parameters that require gradients\n",
    "    # optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr_init)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"max\" if maximize else \"min\",\n",
    "        factor=lr_decay_factor,\n",
    "        patience=scheduler_patience,\n",
    "        threshold=tolerance,\n",
    "        min_lr=min_lr,\n",
    "        verbose=verbose,\n",
    "        threshold_mode=\"abs\",\n",
    "    )\n",
    "    # set the number of iterations over which you would like to accummulate gradients\n",
    "    optim_step_count = (\n",
    "        len(dataloaders[\"train\"].loaders.keys())\n",
    "        if loss_accum_batch_n is None\n",
    "        else loss_accum_batch_n\n",
    "    )\n",
    "    print(f\"optim_step_count = {optim_step_count}\")\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            entity=wandb_entity,\n",
    "            # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "            name=wandb_name,\n",
    "            # Track hyperparameters and run metadata\n",
    "            config={\n",
    "                \"learning_rate\": lr_init,\n",
    "                \"architecture\": wandb_model_config,\n",
    "                \"dataset\": wandb_dataset_config,\n",
    "                \"cur_epochs\": max_iter,\n",
    "                \"starting epoch\": epoch,\n",
    "                \"lr_decay_steps\": lr_decay_steps,\n",
    "                \"lr_decay_factor\": lr_decay_factor,\n",
    "                \"min_lr\": min_lr,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        wandb.define_metric(name=\"Epoch\", hidden=True)\n",
    "        wandb.define_metric(name=\"Batch\", hidden=True)\n",
    "        \n",
    "    print('wandb initialized')\n",
    "\n",
    "    batch_no_tot = 0\n",
    "    ema_values = []\n",
    "    best_validation_correlation = 0\n",
    "    # train over epochs\n",
    "    for epoch, val_obj in early_stopping(\n",
    "        model,\n",
    "        stop_closure,\n",
    "        interval=interval,\n",
    "        patience=patience,\n",
    "        start=epoch,\n",
    "        max_iter=max_iter,\n",
    "        maximize=maximize,\n",
    "        tolerance=tolerance,\n",
    "        restore_best=restore_best,\n",
    "        scheduler=scheduler,\n",
    "        lr_decay_steps=lr_decay_steps,\n",
    "    ):\n",
    "\n",
    "        # executes callback function if passed in keyword args\n",
    "        if cb is not None:\n",
    "            cb()\n",
    "\n",
    "        # train over batches\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        epoch_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "        for batch_no, (data_key, batch) in tqdm(\n",
    "            enumerate(dataloaders[\"train\"]),\n",
    "            total=n_iterations,\n",
    "            desc=\"Epoch {}\".format(epoch),\n",
    "        ):\n",
    "            batch_no_tot += 1\n",
    "            # TODO - polly, these two lines are basically the ones you want to change!\n",
    "            beh = torch.cat([batch['eye_tracker'][:, :, :2].transpose(2, 1), batch['treadmill'][:, :, :2].transpose(2, 1)], axis=1)\n",
    "            video = batch['screen']\n",
    "            b_expanded = beh.unsqueeze(-1).unsqueeze(-1)  # or b[:, :, :, None, None]\n",
    "            # Now broadcast b to match the spatial dimensions [16, 3, 60, 144, 256]\n",
    "            beh_tiled = b_expanded.expand(-1, -1, -1, video.shape[3], video.shape[4])\n",
    "            # Concatenate along dim=1 to get [16, 4, 60, 144, 256]\n",
    "            video = torch.cat([video, beh_tiled], dim=1).to('cuda:0')\n",
    "\n",
    "            resp = batch['responses'].transpose(2, 1).to('cuda:0')\n",
    "            \n",
    "            batch_kwargs = {\n",
    "                'videos': video,\n",
    "                'pupil_center_core': batch['eye_tracker'][:, :, 2:].transpose(2, 1).to('cuda:0'),\n",
    "                'responses': resp\n",
    "            }\n",
    "            batch_args = [video, resp  ]\n",
    "            # batch_args = list(data)\n",
    "            # batch_kwargs = data._asdict() if not isinstance(data, dict) else data\n",
    "            # -----\n",
    "            loss = full_objective(\n",
    "                model,\n",
    "                dataloaders[\"train\"],\n",
    "                data_key,\n",
    "                *batch_args,\n",
    "                **batch_kwargs,\n",
    "                detach_core=detach_core,\n",
    "            )[0]\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.detach()\n",
    "            if (batch_no + 1) % optim_step_count == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "        model.eval()\n",
    "        ###\n",
    "        print(epoch_loss / 225, \"loss\", epoch, \"epoch\")\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        ###\n",
    "        ## after - epoch-analysis\n",
    "\n",
    "        validation_correlation = get_correlations(\n",
    "            model,\n",
    "            dataloaders[\"oracle\"],\n",
    "            device=device,\n",
    "            as_dict=False,\n",
    "            per_neuron=False,\n",
    "            deeplake_ds=False,\n",
    "            flow=model.flow,\n",
    "            cell_coordinates=None,\n",
    "        )\n",
    "\n",
    "        if save_checkpoints:\n",
    "            if validation_correlation > best_validation_correlation:\n",
    "                torch.save(model.state_dict(), f\"{checkpoint_save_path}best.pth\")\n",
    "                best_validation_correlation = validation_correlation\n",
    "\n",
    "        if loss_function == \"PoissonLoss\" or (not model.latent):\n",
    "            val_loss = full_objective(\n",
    "                model,\n",
    "                dataloaders[\"oracle\"],\n",
    "                data_key,\n",
    "                *batch_args,\n",
    "                **batch_kwargs,\n",
    "                detach_core=detach_core,\n",
    "            )\n",
    "        else:\n",
    "            val_loss, kl_div = full_objective(\n",
    "                model,\n",
    "                dataloaders[\"oracle\"],\n",
    "                data_key,\n",
    "                *batch_args,\n",
    "                **batch_kwargs,\n",
    "                detach_core=detach_core,\n",
    "            )\n",
    "\n",
    "        # torch.save(\n",
    "        # model.state_dict(), f\"toymodels2/temp_save.pth\"\n",
    "        # )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}, Batch {batch_no}, Train loss {loss}, Validation loss {val_loss}\"\n",
    "        )\n",
    "        print(f\"EPOCH={epoch}  validation_correlation={validation_correlation}\")\n",
    "\n",
    "        ema_values.append(validation_correlation)\n",
    "        ema = calculate_ema(torch.tensor(ema_values), ema_span)[-1]\n",
    "        if use_wandb:\n",
    "            wandb_dict = {\n",
    "                \"Epoch Train loss\": epoch_loss,\n",
    "                \"Batch\": batch_no_tot,\n",
    "                \"Epoch\": epoch,\n",
    "                \"validation_correlation\": validation_correlation,\n",
    "                # \"log_det\": log_det,\n",
    "                \"Epoch validation loss\": val_loss,\n",
    "                \"EMA validation loss\": ema,\n",
    "                # \"Poisson Loss\": pos_loss,\n",
    "                \"ZIG Loss\": val_loss,\n",
    "                \"Epoch\": epoch,\n",
    "                \"Learning rate\": lr,\n",
    "            }\n",
    "            wandb.log(wandb_dict)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    ##### Model evaluation ####################################################################################################\n",
    "    model.eval()\n",
    "    # if save_checkpoints:\n",
    "    # torch.save(model.state_dict(), f\"{checkpoint_save_path}final.pth\")\n",
    "\n",
    "    # Compute avg validation and test correlation\n",
    "    validation_correlation = get_correlations(\n",
    "        model,\n",
    "        dataloaders[\"oracle\"],\n",
    "        device=device,\n",
    "        as_dict=False,\n",
    "        per_neuron=False,\n",
    "        deeplake_ds=False,\n",
    "        flow=model.flow,\n",
    "        cell_coordinates=None,\n",
    "    )\n",
    "    print(f\"\\n\\n FINAL validation_correlation {validation_correlation} \\n\\n\")\n",
    "\n",
    "    output = {}\n",
    "    output[\"validation_corr\"] = validation_correlation\n",
    "\n",
    "    score = np.mean(validation_correlation)\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    # removing the checkpoints except the last one\n",
    "    # to_clean = os.listdir(checkpoint_save_path)\n",
    "    to_clean = os.listdir(\"toymodels\")\n",
    "    for f2c in to_clean:\n",
    "        if \"epoch\" in f2c:\n",
    "            os.remove(os.path.join(\"toymodels\", f2c))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodality_config\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m60\u001b[39m \n\u001b[1;32m     13\u001b[0m cfg\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmodality_config\u001b[38;5;241m.\u001b[39mscreen\u001b[38;5;241m.\u001b[39mvalid_condition \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtier\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m---> 15\u001b[0m val_dl \u001b[38;5;241m=\u001b[39m \u001b[43mget_multisession_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/vast-nhr/home/turishcheva/u14642/konsti_experanto/experanto/experanto/dataloaders.py:50\u001b[0m, in \u001b[0;36mget_multisession_dataloader\u001b[0;34m(paths, configs, shuffle_keys, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 50\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mChunkDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     dataloaders[dataset_name] \u001b[38;5;241m=\u001b[39m MultiEpochsDataLoader(dataset,\n\u001b[1;32m     52\u001b[0m                                                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdataloader,)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LongCycler(dataloaders)\n",
      "File \u001b[0;32m/mnt/vast-nhr/home/turishcheva/u14642/konsti_experanto/experanto/experanto/datasets.py:359\u001b[0m, in \u001b[0;36mChunkDataset.__init__\u001b[0;34m(self, root_folder, global_sampling_rate, global_chunk_size, add_behavior_as_channels, replace_nans_with_means, cache_data, out_keys, normalize_timestamps, modality_config, seed)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_screen_sample_times \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_time, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_rates[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    357\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# iterate over the valid condition in modality_config[\"screen\"][\"valid_condition\"] to get the indices of self._screen_sample_times that meet all criteria\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_valid_sample_times_filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_full_valid_sample_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_for_valid_intervals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# self._full_valid_sample_times_unfiltered = self.get_full_valid_sample_times(filter_for_valid_intervals=False)\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# the _valid_screen_times are the indices from which the starting points for the chunks will be taken\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# sampling stride is used to reduce the number of starting points by the stride\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# default of stride is 1, so all starting points are used\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_screen_times \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_valid_sample_times_filtered[::\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_stride]\n",
      "File \u001b[0;32m/mnt/vast-nhr/home/turishcheva/u14642/konsti_experanto/experanto/experanto/datasets.py:602\u001b[0m, in \u001b[0;36mChunkDataset.get_full_valid_sample_times\u001b[0;34m(self, filter_for_valid_intervals)\u001b[0m\n\u001b[1;32m    599\u001b[0m     additional_valid_conditions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtier\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank\u001b[39m\u001b[38;5;124m\"\u001b[39m}  \n\u001b[1;32m    600\u001b[0m     valid_conditions\u001b[38;5;241m.\u001b[39mappend(additional_valid_conditions)\n\u001b[0;32m--> 602\u001b[0m sample_mask_from_meta_conditions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_screen_sample_mask_from_meta_conditions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_for_valid_intervals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m final_mask \u001b[38;5;241m=\u001b[39m duration_mask \u001b[38;5;241m&\u001b[39m sample_mask_from_meta_conditions\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_screen_sample_times[possible_indices[final_mask]]\n",
      "File \u001b[0;32m/mnt/vast-nhr/home/turishcheva/u14642/konsti_experanto/experanto/experanto/datasets.py:570\u001b[0m, in \u001b[0;36mChunkDataset.get_screen_sample_mask_from_meta_conditions\u001b[0;34m(self, satisfy_for_next, valid_conditions_sum_of_product, filter_for_valid_intervals)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m interval \u001b[38;5;129;01min\u001b[39;00m valid_intervals:\n\u001b[1;32m    569\u001b[0m         mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_screen_sample_times \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m interval\u001b[38;5;241m.\u001b[39mstart) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_screen_sample_times \u001b[38;5;241m<\u001b[39m interval\u001b[38;5;241m.\u001b[39mend)\n\u001b[0;32m--> 570\u001b[0m         sample_mask \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m mask\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m satisfy_for_next \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    573\u001b[0m     windows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mstride_tricks\u001b[38;5;241m.\u001b[39msliding_window_view(sample_mask, satisfy_for_next)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg['dataset']['modality_config']['responses']['sampling_rate'] = 30\n",
    "cfg['dataset']['modality_config']['responses']['chunk_size'] = 60\n",
    "\n",
    "cfg['dataset']['modality_config']['eye_tracker']['sampling_rate'] = 30\n",
    "cfg['dataset']['modality_config']['eye_tracker']['chunk_size'] = 60\n",
    "\n",
    "cfg['dataset']['modality_config']['treadmill']['sampling_rate'] = 30\n",
    "cfg['dataset']['modality_config']['treadmill']['chunk_size'] = 60\n",
    "\n",
    "cfg['dataset']['modality_config']['screen']['sampling_rate'] = 30\n",
    "cfg['dataset']['modality_config']['screen']['chunk_size'] =  60 \n",
    "\n",
    "cfg.dataset.modality_config.screen.valid_condition = {\"tier\": \"validation\"}\n",
    "\n",
    "val_dl = get_multisession_dataloader(full_paths, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mew\n"
     ]
    }
   ],
   "source": [
    "print('mew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/turishcheva/u14642/miniconda/envs/latent_model_finn/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = train_dl\n",
    "# todo - undo it after the validation set labels are updated\n",
    "# dataloaders[\"oracle\"] = val_dl\n",
    "dataloaders[\"oracle\"] = {}\n",
    "for m in full_paths:\n",
    "    dataloaders[\"oracle\"][m.split('dynamic')[-1].split('-Video')[0]] = get_multisession_dataloader([m], cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_inint = 5e-3\n",
    "min_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ZIGEncoder(\n",
       "  (core): Factorized3dCore(\n",
       "    (_input_weight_regularizer): LaplaceL2norm(\n",
       "      (laplace): Laplace()\n",
       "    )\n",
       "    (temporal_regularizer): DepthLaplaceL21d(\n",
       "      (laplace): Laplace1d()\n",
       "    )\n",
       "    (features): Sequential(\n",
       "      (layer0): Sequential(\n",
       "        (conv_spatial): Conv3d(4, 32, kernel_size=(1, 11, 11), stride=(1, 1, 1))\n",
       "        (conv_temporal): Conv3d(32, 32, kernel_size=(11, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(32, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "      (layer1): Sequential(\n",
       "        (conv_spatial_1): Conv3d(32, 64, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "        (conv_temporal_1): Conv3d(64, 64, kernel_size=(5, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(64, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (conv_spatial_2): Conv3d(64, 128, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "        (conv_temporal_2): Conv3d(128, 128, kernel_size=(5, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(128, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "    )\n",
       "  ) [Factorized3dCore regularizers: gamma_input_spatial = 10|gamma_input_temporal = 0.01]\n",
       "  \n",
       "  (readout): MultipleFullGaussian2d(\n",
       "    (29156-11-10): full FullGaussian2d (128 x 18 x 46 -> 7440)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29228-2-10): full FullGaussian2d (128 x 18 x 46 -> 7928)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29234-6-9): full FullGaussian2d (128 x 18 x 46 -> 8285)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29513-3-5): full FullGaussian2d (128 x 18 x 46 -> 7671)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29514-2-9): full FullGaussian2d (128 x 18 x 46 -> 7495)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (26872-17-20): full FullGaussian2d (128 x 18 x 46 -> 7776)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (27204-5-13): full FullGaussian2d (128 x 18 x 46 -> 7538)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29515-10-12): full FullGaussian2d (128 x 18 x 46 -> 7863)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29623-4-9): full FullGaussian2d (128 x 18 x 46 -> 7908)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29647-19-8): full FullGaussian2d (128 x 18 x 46 -> 8202)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29712-5-9): full FullGaussian2d (128 x 18 x 46 -> 7939)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "    (29755-2-8): full FullGaussian2d (128 x 18 x 46 -> 8122)  -> Conv2d(1, 2, kernel_size=(11, 5), stride=(1, 1), padding=(5, 2))\n",
       "    \n",
       "  )\n",
       "  (shifter): MLPShifter(\n",
       "    (29156-11-10): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29228-2-10): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29234-6-9): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29513-3-5): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29514-2-9): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (26872-17-20): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (27204-5-13): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29515-10-12): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29623-4-9): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29647-19-8): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29712-5-9): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "    (29755-2-8): MLP(\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "        (1): Tanh()\n",
       "        (2): Linear(in_features=5, out_features=5, bias=True)\n",
       "        (3): Tanh()\n",
       "        (4): Linear(in_features=5, out_features=2, bias=True)\n",
       "        (5): Tanh()\n",
       "      )\n",
       "    ) [MLP regularizers: ]\n",
       "    \n",
       "  )\n",
       "  (logloc): ParameterDict(\n",
       "      (26872-17-20): Parameter containing: [torch.cuda.FloatTensor of size 1x7776 (cuda:0)]\n",
       "      (27204-5-13): Parameter containing: [torch.cuda.FloatTensor of size 1x7538 (cuda:0)]\n",
       "      (29156-11-10): Parameter containing: [torch.cuda.FloatTensor of size 1x7440 (cuda:0)]\n",
       "      (29228-2-10): Parameter containing: [torch.cuda.FloatTensor of size 1x7928 (cuda:0)]\n",
       "      (29234-6-9): Parameter containing: [torch.cuda.FloatTensor of size 1x8285 (cuda:0)]\n",
       "      (29513-3-5): Parameter containing: [torch.cuda.FloatTensor of size 1x7671 (cuda:0)]\n",
       "      (29514-2-9): Parameter containing: [torch.cuda.FloatTensor of size 1x7495 (cuda:0)]\n",
       "      (29515-10-12): Parameter containing: [torch.cuda.FloatTensor of size 1x7863 (cuda:0)]\n",
       "      (29623-4-9): Parameter containing: [torch.cuda.FloatTensor of size 1x7908 (cuda:0)]\n",
       "      (29647-19-8): Parameter containing: [torch.cuda.FloatTensor of size 1x8202 (cuda:0)]\n",
       "      (29712-5-9): Parameter containing: [torch.cuda.FloatTensor of size 1x7939 (cuda:0)]\n",
       "      (29755-2-8): Parameter containing: [torch.cuda.FloatTensor of size 1x8122 (cuda:0)]\n",
       "  )\n",
       "  (logk): ParameterDict(\n",
       "      (26872-17-20): Parameter containing: [torch.cuda.FloatTensor of size 1x7776 (cuda:0)]\n",
       "      (27204-5-13): Parameter containing: [torch.cuda.FloatTensor of size 1x7538 (cuda:0)]\n",
       "      (29156-11-10): Parameter containing: [torch.cuda.FloatTensor of size 1x7440 (cuda:0)]\n",
       "      (29228-2-10): Parameter containing: [torch.cuda.FloatTensor of size 1x7928 (cuda:0)]\n",
       "      (29234-6-9): Parameter containing: [torch.cuda.FloatTensor of size 1x8285 (cuda:0)]\n",
       "      (29513-3-5): Parameter containing: [torch.cuda.FloatTensor of size 1x7671 (cuda:0)]\n",
       "      (29514-2-9): Parameter containing: [torch.cuda.FloatTensor of size 1x7495 (cuda:0)]\n",
       "      (29515-10-12): Parameter containing: [torch.cuda.FloatTensor of size 1x7863 (cuda:0)]\n",
       "      (29623-4-9): Parameter containing: [torch.cuda.FloatTensor of size 1x7908 (cuda:0)]\n",
       "      (29647-19-8): Parameter containing: [torch.cuda.FloatTensor of size 1x8202 (cuda:0)]\n",
       "      (29712-5-9): Parameter containing: [torch.cuda.FloatTensor of size 1x7939 (cuda:0)]\n",
       "      (29755-2-8): Parameter containing: [torch.cuda.FloatTensor of size 1x8122 (cuda:0)]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zig_model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['responses'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIGLoss\n",
      "optim_step_count = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/turishcheva/u14642/miniconda/envs/latent_model_finn/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpollytur\u001b[0m (\u001b[33mecker-lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "cat: /sys/module/amdgpu/initstate: No such file or directory\n",
      "ERROR:root:Driver not initialized (amdgpu not found in modules)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/vast-nhr/home/turishcheva/u14642/finn_model_with_experanto/latent_space_model/wandb/run-20250418_224148-xqj3106q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ecker-lab/finn_mode_with_experanto/runs/xqj3106q/workspace' target=\"_blank\">16_core_channels_latent_mice6_10</a></strong> to <a href='https://wandb.ai/ecker-lab/finn_mode_with_experanto' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ecker-lab/finn_mode_with_experanto' target=\"_blank\">https://wandb.ai/ecker-lab/finn_mode_with_experanto</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ecker-lab/finn_mode_with_experanto/runs/xqj3106q/workspace' target=\"_blank\">https://wandb.ai/ecker-lab/finn_mode_with_experanto/runs/xqj3106q/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb initialized\n"
     ]
    }
   ],
   "source": [
    "validation_score = standard_trainer(\n",
    "    zig_model,\n",
    "    dataloaders,\n",
    "    111,\n",
    "    use_wandb=True,\n",
    "    wandb_name=\"16_core_channels_latent_mice6_10\",\n",
    "    loss_function=\"ZIGLoss\",\n",
    "    # loss_function= \"PoissonLoss\",\n",
    "    verbose=True,\n",
    "    lr_decay_steps=4,\n",
    "    lr_init=lr_inint,\n",
    "    min_lr=min_lr,\n",
    "    device=device,\n",
    "    patience=12,  # 12#8,\n",
    "    scheduler_patience=10,  # 10#6,\n",
    "    checkpoint_save_path=\"./test_training\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latent_model_finn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
